{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from numpy import dot, inner\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "print(cancer.data.shape)\n",
    "\n",
    "#build dataframe\n",
    "import pandas as pd\n",
    "X = pd.DataFrame(cancer['data'])\n",
    "y = cancer ['target']\n",
    "X.describe()\n",
    "\n",
    "# separate between train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,stratify=cancer.target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest classifier \n",
    "param_grid = {'max_depth': [1,2,3,4,5,6,None], 'max_features': [3,5,7,10, None]}\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, scoring= 'roc_auc', cv=5,return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.3f}\".format(grid_search.best_score_))\n",
    "print(\"Test set score: {:.3f}\".format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_depth= 6, max_features= 5)\n",
    "np.set_printoptions(precision=3)\n",
    "rfc.fit(X_train,y_train)\n",
    "y_pred=rfc.predict(X_test)\n",
    "roc_test=roc_auc_score(y_test, y_pred)\n",
    "print(\"accuracy on training set is {:.3f}\".format(rfc.score(X_train,y_train)))\n",
    "print(\"accuracy on test set is {:.3f}\".format(rfc.score(X_test,y_test)))\n",
    "print(\"roc_auc_score on test set is {:.3f}\".format(roc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boosting \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "param_grid = {'max_depth': [1,3,5,None],'max_features': [1,2,3,5,10,None],'learning_rate' : [0.03,0.1,0.5]}\n",
    "grid_search = GridSearchCV(GradientBoostingClassifier(), param_grid,scoring= 'roc_auc', cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.3f}\".format(grid_search.best_score_))\n",
    "print(\"Test set score: {:.3f}\".format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's fit the best parameters on the whole training set\n",
    "gbr = GradientBoostingClassifier(max_depth=5, max_features=2,learning_rate=0.03)\n",
    "np.set_printoptions(precision=3)\n",
    "gbr.fit(X_train,y_train)\n",
    "y_pred=gbr.predict(X_test)\n",
    "roc_test=roc_auc_score(y_test, y_pred)\n",
    "print(\"accuracy on the training set is {:.3f}\".format(gbr.score(X_train,y_train)))\n",
    "print(\"accuracy on the test set is {:.3f}\".format(gbr.score(X_test,y_test)))\n",
    "print(\"roc_auc_score on test set is {:.3f}\".format(roc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingClassifier()\n",
    "np.set_printoptions(precision=3)\n",
    "gbr.fit(X_train,y_train)\n",
    "y_pred=gbr.predict(X_test)\n",
    "roc_test=roc_auc_score(y_test, y_pred)\n",
    "print(\"accuracy on the training set is {:.3f}\".format(gbr.score(X_train,y_train)))\n",
    "print(\"accuracy on the test set is {:.3f}\".format(gbr.score(X_test,y_test)))\n",
    "print(\"roc_auc_score on test set is {:.3f}\".format(roc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= pd.DataFrame(X_train)\n",
    "echmean=X_train.mean(axis=0)\n",
    "echcov= X_train.cov()\n",
    "X_virt= np.random.multivariate_normal(echmean, echcov, 100000, check_valid ='warn')\n",
    "d_virt=pd.DataFrame(X_virt, columns=X_train.columns)\n",
    "d_virt.describe()\n",
    "\n",
    "y_virt=gbr.predict(d_virt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "for i in range (4,10):\n",
    "    clf = DecisionTreeClassifier(max_depth=i,random_state=42)\n",
    "    score=cross_val_score(clf,d_virt,y_virt,cv=5,scoring= 'roc_auc').mean() \n",
    "    print(\"k=\",i,\"average cross-validation score: {:.3f}\".format(score))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=7\n",
    "clf= DecisionTreeClassifier(max_depth=k)\n",
    "clf.fit(d_virt,y_virt)\n",
    "Acc_appr=clf.score(X_train,y_train)\n",
    "Acc_test=clf.score(X_test,y_test)\n",
    "y_pred=clf.predict(X_test)\n",
    "roc_test=roc_auc_score(y_test, y_pred)\n",
    "print('for depth',k)\n",
    "print(\"accuracy on training set is {:.3f}\".format(Acc_appr))\n",
    "print(\"accuracy on test set is {:.3f}\".format(Acc_test))\n",
    "print(\"roc_auc_score on test set is {:.3f}\".format(roc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary function\n",
    "\n",
    "def make_exp_vec(exp):\n",
    "    \"\"\"\n",
    "    Takes a LIME explanation which is a dictionary (i, w(i)) where i is the feature id and w(i) is the weight \n",
    "    of the ith feature. With the explanation vector we generate, we can \n",
    "    \n",
    "    Arg - \n",
    "        exp: LIME explanation\n",
    "    \n",
    "    Returns - \n",
    "        v: explanation vector where v[i] = w(i)\n",
    "    \"\"\"\n",
    "    k = list(exp.keys())[0]\n",
    "    l = exp[k]\n",
    "    v = np.zeros(len(l))\n",
    "    \n",
    "    for (i,w) in l:\n",
    "        v[i] = w\n",
    "    \n",
    "    return v\n",
    "\n",
    "def exp_point(point, data, fn, f_names, c_names):\n",
    "    \"\"\"\n",
    "    The explanation vector for top label of a point in a certain data set.\n",
    "    \n",
    "    Arg - \n",
    "        point: point to explain\n",
    "        data: data model was trained on\n",
    "        fn: probability black box\n",
    "        f_names: names of features\n",
    "        c_names: names of classes\n",
    "        \n",
    "    Returns - \n",
    "        explanation vector (see make_exp_vec())\n",
    "    \"\"\"\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(data, feature_names=f_names, class_names=c_names, discretize_continuous=False)\n",
    "    exp = explainer.explain_instance(point, fn, num_features=len(point), top_labels=1)\n",
    "    \n",
    "    v = make_exp_vec(exp.local_exp)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanation similarity for gbr and clf\n",
    "points_id = np.random.choice(len(cancer['data']), size=120, replace=False)\n",
    "points = cancer['data'][points_id]\n",
    "\n",
    "sim = []\n",
    "for x in points:\n",
    "    \n",
    "    v1 = exp_point(x, cancer['data'], gbr.predict_proba, cancer.feature_names, cancer.target_names)\n",
    "    v2 = exp_point(x, cancer['data'], clf.predict_proba, cancer.feature_names, cancer.target_names)\n",
    "    \n",
    "    sim.append(np.inner(v1, v2) / (norm(v1) * norm(v2)))\n",
    "    \n",
    "num_bins = 50\n",
    "sim_plus = [x for x in sim if x>=0]\n",
    "num_neg = len([x for x in sim if x<0])\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Similarity Between Explanations')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Freq.')\n",
    "n, bins, patches = plt.hist(sim_plus, num_bins, facecolor='tab:blue', alpha=0.5)\n",
    "plt.show()\n",
    "print('number of negatively similar vectors: ', num_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanation similarity for rf and clf\n",
    "points_id = np.random.choice(len(cancer['data']), size=120, replace=False)\n",
    "points = cancer['data'][points_id]\n",
    "\n",
    "sim = []\n",
    "for x in points:\n",
    "    \n",
    "    v1 = exp_point(x, cancer['data'], rfc.predict_proba, cancer.feature_names, cancer.target_names)\n",
    "    v2 = exp_point(x, cancer['data'], clf.predict_proba, cancer.feature_names, cancer.target_names)\n",
    "    \n",
    "    sim.append(np.inner(v1, v2) / (norm(v1) * norm(v2)))\n",
    "    \n",
    "num_bins = 50\n",
    "sim_plus = [x for x in sim if x>=0]\n",
    "num_neg = len([x for x in sim if x<0])\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Similarity Between Explanations')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Freq.')\n",
    "n, bins, patches = plt.hist(sim_plus, num_bins, facecolor='tab:blue', alpha=0.5)\n",
    "plt.show()\n",
    "print('number of negatively similar vectors: ', num_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
